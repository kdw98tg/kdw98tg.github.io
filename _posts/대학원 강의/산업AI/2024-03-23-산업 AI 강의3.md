---
title: "[대학원 강의]20240316 산업 AI 강의 정리"

categories:
  -  Collage
  
tags:
  - [AI, 산업AI]

toc: true
toc_sticky: true

published: true

date: 2024-03-23
last_modified_at: 2024-03-24
---

지금의 AI 에서는 Transfer Learning(전이 학습), 오토 인코더, Meta Learning을 제일 많이 쓴다고 합니다.

지금 우리가 사용하고 있는 생성형 AI는 사실 생성형이 아니라, Generative AI 라고 합니다. 이게 모델이 진짜로 창의적인것들을 만들어 낸다면, 사용자 입장에서는 서비스에 대한 만족도가 높지 못하기 때문이라고 합니다.

그래서 오늘 배운 것은 `Attention Mechanism` 입니다. 이것은 Generative AI에서 사람과 대화하기 위한 자연어 처리를 하는 기법중에 하나입니다.

우선 `Attention Mechanism`에 대해 정리하기 전에, seq2seq이 뭔지 알아봅시다.


## Seq2Seq 이란?
> seq2seq(sequence-to-sequence) 모델은 한 시퀀스를 다른 시퀀스로 변환하는 작업을 수행하는 딥러닝 모델 입니다. 주로 자연어 처리 분야에서 활용 됩니다.

이 모델은 인코더와 디코더라는 모듈을 가지고 있습니다. 그래서 seq2seq를 Encoder-Decoder 모델이라고도 합니다.

이 두 모듈이 서로 협력하여 입력 시퀀스를 원하는 출력 시퀀스로 변환 합니다.

![seq2seq](/images/Pasted%20image%2020240324225136.png)

인코더는 입력 데이터를 인코딩하고, 디코딩은 인코딩된 데이터를 디코딩 합니다. 쉽게 말하자면, 자연어가 들어가면, Encoder 가 컴퓨터가 이해할 수 있는 숫자로 변환시키게 됩니다.

디코더는 인코딩과 반대로 다시 원래의 형태로 되돌려 주는 과정을 말합니다. Encoder 에서 컴퓨터가 이해할 수 있는 숫자로 변환된 것을 다시 인간이 이해할 수 있는 형태로 변환 시키는 것이죠.

![encoder 과 decoder](/images/Pasted%20image%2020240324225355.png)

### 인코더
인코더는 일반적으로 RNN(Recurrent Neural Network)이나 LSTM(Long Short-Term Memory), GRU(Gated Recurrent Unit)등의 순환 신경망 구조를 사용하여 입력 시퀀스를 `고정 길이의 벡터(Fixed Length Vector == Context Vector)`로 변환하는 역할을 수행 합니다. 

이러한 구조는 시퀀스 데이터를 처리하는데 적합하며, 순차적인 정보를 효과적으로 인코딩할 수 있습니다.


### 디코더
디코더는 인코더의 출력인 고정 길이의 벡터를 기반으로 원하는 출력 시퀀스를 생성하는 역할을 수행합니다. 디코더 역시 RNN, LSTM, GRU 등의 순환 신경망 구조를 사용합니다.

기존 순환신경망 구조와 다른 점은 벡터 h를 입력으로 받는다는 것입니다.

디코더의 예측은 일반적으로 소프트맥스 활성화 함수를 통해 확률 분포로 변환되며, 가장 확률이 높은 단어가 선택 됩니다. 이 과정을 반복하여 최종적으로 출력 시퀀스가 생성 됩니다.


인코더와 디코더를 모두 거치는 과정 즉, seq2seq의 약도는 다음과 같습니다.
![seq2seq](/images/Pasted%20image%2020240324225820.png)


### seq2seq의 한계
인코더가 입력 시퀀스를 하나의 고정된 길이의 벡터로 압축 합니다. 이로 인해 입력 시퀀스의 길이가 길어질수록 정보의 손실이 발생할 수 있습니다. 또한 RNN의 고질적인 문제인 `경사소실` 또는 `폭발문제`가 존재한다는 것입니다. 이를 극복하기 위해 제안된 방법이 `Attention Mechanism`입니다.


## Attention Mechanism

요즘 AI를 좀 한다는 사람은 최근에 다 이것을 하고 있습니다. `Attention Mechanism`은 자연어 처리 를 잘 풀기 위해 고안되었습니다.

### Attention Mechanism 이란?
seq2seq 모델은 기존의 RNN 모델과 같이 고정된 길이의 벡터를 입력으로 받아 고정된 길이의 벡터를 출력하는 구조를 가지고 있었습니다. 그러나, 이 구조는 고정된 길이의 벡터 압축으로 시퀀스의 길이가 길어질수록 정보의 손실이 발생할 수 있다는 단점이 있었습니다.

이러한 한계를 보완하기 위해 Attention Mechanism이 도입되었습니다. Attention Mechanism은 입력 문장의 모든 단어를 동일한 가중치로 취급하지 않고, 출력 문장에서 특정 위치에 대응하는 입력 단어들에 더 많은 가중치를 부여합니다. 이를 통해 입출려과 출력의 길이가 다른 경우에도 모델이 더욱 정확하고 유연하게 작동할 수 있게 됩니다.

간단하게 이야기해서, 기존에는 자연어처리가 되긴했는데 그 문장의 순서가 뒤죽박죽이라서, 사용자들이 어색함을 느꼈다면, `Attention Mechanism`이 도입되고 난 후, 순서에 가중치를 두어 재배치 함으로써, 더욱 자연어 처리를 매끄럽게 할 수 있게 되었습니다.

그리하여, 자연어처리는 이 `Attention Mechanism`이 도입 된 시점과 그렇지 못한 시점으로 나뉘게 된다고 합니다.

더욱 자세한 내용이 있지만, `Attention Mechanism`에 대한 더 자세한 이야기는 다른 포스팅에서 하도록 하겠습니다.
